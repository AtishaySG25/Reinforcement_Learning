{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KjAwMCTfSUjs"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mutfQDpOWDph"
      },
      "source": [
        "  ## Grid world\n",
        "  It has N x M states. The Agent can move from one state to another state depending on the state it is currently in and the action it takes.\n",
        "\n",
        "  Here, there are total of 5*5 = 25 states representing the \"environment\".\n",
        "  \n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |(0,0)|(0,1)|(0,2)|(0,3)|(0,4)|\n",
        "  | 1 |(1,0)|(1,1)|(1,2)|(1,3)|(1,4)|\n",
        "  | 2 |(2,0)|(2,1)|(2,2)|(2,3)|(2,4)|\n",
        "  | 3 |(3,0)|(3,1)|(3,2)|(3,3)|(3,4)|\n",
        "  | 4 |(4,0)|(4,1)|(4,2)|(4,3)|(4,4)|\n",
        "  \n",
        "____________________________\n",
        "### Example\n",
        "**agent is at (2,2)**:\n",
        "\n",
        "\n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |     |     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |[ A ]|     |     |\n",
        "  | 3 |     |     |     |     |     |\n",
        "  | 4 |     |     |     |     | END |\n",
        "\n",
        "\n",
        "Takes an action to move ***right***. Action: (0,1)\n",
        "\n",
        "next_state = current_state + action = (2,2) + (0,1) = (2,3)\n",
        "\n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |     |     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |     |[ A ]|     |\n",
        "  | 3 |     |     |     |     |     |\n",
        "  | 4 |     |     |     |     | END |\n",
        "\n",
        "\n",
        "Takes an action to move ***down***. Action: (1,0)\n",
        "\n",
        "next_state = current_state + action = (2,3) + (1,3) = (3,3)\n",
        "\n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |     |     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |     |     |     |\n",
        "  | 3 |     |     |     |[ A ]|     |\n",
        "  | 4 |     |     |     |     | END |\n",
        "\n",
        "\n",
        "Takes an action to move ***right***. Action: (0,1)\n",
        "\n",
        "next_state = current_state + action = (3,3) + (0,1) = (3,4)\n",
        "\n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |     |     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |     |     |     |\n",
        "  | 3 |     |     |     |     |[ A ]|\n",
        "  | 4 |     |     |     |     | END |\n",
        "\n",
        "\n",
        "Takes an action to move ***down***. Action: (1,0)\n",
        "\n",
        "next_state = current_state + action = (3,4) + (1,0) = (4,4)\n",
        "\n",
        "  |   |  0  |  1  |  2  |  3  |  4  |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |     |     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |     |     |     |\n",
        "  | 3 |     |     |     |     |     |\n",
        "  | 4 |     |     |     |     |[ A ]|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "99Xjz2tSTbyL"
      },
      "outputs": [],
      "source": [
        "class GridWorldEnv:\n",
        "  \"\"\"Grid world is an one of most classical reinforcement learning problems.\n",
        "  Here the envirnment is made of a rectangular 2-d box with N rows and M columns,\n",
        "  totally consisting of N x M states.\n",
        "\n",
        "  The agent at any given point can take one of the 4 possible actions - to move:\n",
        "  - Left (0,-1)\n",
        "  - Right (0,1)\n",
        "  - Up (-1,0)\n",
        "  - Down (1,0)\n",
        "\n",
        "  Different types of rewards and constraints can be formulated in this kind of\n",
        "  setup\n",
        "\n",
        "  Reward\n",
        "  ------\n",
        "  - Moving to the terminal state receives a reward of +10\n",
        "  - Every other step receives -1 reward.\n",
        "\n",
        "  Teminal states:\n",
        "  (4,4)\n",
        "\n",
        "  |   | 0   | 1   | 2   | 3   | 4   |\n",
        "  |---|-----|-----|-----|-----|-----|\n",
        "  | 0 |start|     |     |     |     |\n",
        "  | 1 |     |     |     |     |     |\n",
        "  | 2 |     |     |     |     |     |\n",
        "  | 3 |     |     |     |     |     |\n",
        "  | 4 |     |     |     |     | END |\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, N = 10, M = 10):\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "    self.total_states = N*M\n",
        "\n",
        "    self.observation_space = list(itertools.product(range(N), range(M)))\n",
        "    self.action_space = [(0,1), (1,0), (0, -1), (-1, 0)]\n",
        "\n",
        "    self.terminated = False\n",
        "    self.total_reward = 0\n",
        "\n",
        "    # once the agent reaches terminal state, it stays there. Assume start state cannot be terminal state.\n",
        "    # also known as absorbing states\n",
        "    self.terminal_states = [(4,4)]\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = (0,0)\n",
        "\n",
        "  def _get_transition_probability(self, present_state, action, next_state):\n",
        "\n",
        "    if present_state in self.terminal_states:\n",
        "      return 0\n",
        "\n",
        "    # If the expected state matches the action taken, return a probability 1\n",
        "    expected_state = tuple(np.array(present_state) + np.array(action))\n",
        "    if expected_state == next_state:\n",
        "      return 1\n",
        "\n",
        "    # Make sure the agent does not go out of the grid\n",
        "    if (expected_state not in self.observation_space\n",
        "        and present_state == next_state):\n",
        "      return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "  def _get_reward(self,present_state, action, next_state):\n",
        "    if next_state in self.terminal_states:\n",
        "      reward = 10\n",
        "    else:\n",
        "      reward = -1\n",
        "    return reward\n",
        "\n",
        "  def step(self, action):\n",
        "    # check if terminal state is reached\n",
        "    if self.state in self.terminal_states:\n",
        "      self.terminated = True\n",
        "      reward = np.nan\n",
        "      return self.state, reward, self.terminated\n",
        "\n",
        "    max_prob = -np.inf\n",
        "    for possible_state in self.observation_space:\n",
        "      p = self._get_transition_probability(self.state, action, possible_state)\n",
        "      if p > max_prob:\n",
        "        next_state = possible_state\n",
        "        max_prob = p\n",
        "\n",
        "    reward = self._get_reward(self.state, action, next_state)\n",
        "\n",
        "    self.state = next_state\n",
        "    self.total_reward = self.total_reward + reward\n",
        "\n",
        "    return self.state, reward, self.terminated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "IMBgnuazTpYJ"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  def __init__(self,env):\n",
        "    self.env = env\n",
        "    env.reset()\n",
        "\n",
        "  def policy(self):\n",
        "    current_state = self.env.state\n",
        "    action = random.choice([(0,1), (1,0)])\n",
        "    # action = (0, 1)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZLSw_TWUK5n",
        "outputId": "94aeed25-0709-493b-ea61-e914effe4f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = GridWorldEnv(5,5)\n",
        "agent = Agent(env)\n",
        "\n",
        "agent.policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkRxBU_1U0fy",
        "outputId": "6cc873fe-e7a2-46be-c03b-c091b2d0b1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glohNV6arLbg",
        "outputId": "03e473b0-aea9-42b3-8af2-d4ab61ed0035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAfeVL7QqdOW",
        "outputId": "162f64ae-b81f-4765-c61a-b4755dad5732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((0, -1), ((4, 4), nan, True))"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = agent.policy()\n",
        "a, env.step(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_bi0j6FUXUI",
        "outputId": "f3037b1d-91a6-478c-e6ef-f659f030a0b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averahe total reward:  11.09\n"
          ]
        }
      ],
      "source": [
        "# random walk\n",
        "# get total reward\n",
        "# try with different policies\n",
        "env = GridWorldEnv(5,5)\n",
        "agent = Agent(env)\n",
        "\n",
        "episodes = 100\n",
        "reward_sum = 0\n",
        "for i in range(episodes):\n",
        "  env = GridWorldEnv(5,5)\n",
        "  agent = Agent(env)\n",
        "  while not env.terminated:\n",
        "    current_state = env.state\n",
        "    action = agent.policy()\n",
        "    next_state, reward, terminated = env.step(action)\n",
        "    #print(\"current_state:\",current_state, \" action: \", action, \"next_state: \", next_state, \"reward: \", reward)\n",
        "    reward_sum += 1\n",
        "print(\"Averahe total reward: \", reward_sum/episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpfmG2w_YVVI",
        "outputId": "9e75c8f5-c7a0-4082-a974-2e1be40367e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-70"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBteaFofd_-1"
      },
      "source": [
        "### 📔 Tasks\n",
        "- Obtain the average total reward received by taking random actions.\n",
        "- Add dummy states as [(3,0), (2,3)]. Dummy states are states where the agents cannot move to. Hint: modify the transition probability such that the agent cannot move to these states, similar to how we control the agent from moving out of the grid.\n",
        "- Does this change the average total reward received by the agent.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
