{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "k4G7j4e-oL6F"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NDed4dkEoorn"
      },
      "outputs": [],
      "source": [
        "class GridWorldEnv:\n",
        "  def __init__(self, N = 10, M = 10):\n",
        "    # States of a gridworld\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "\n",
        "    # state space\n",
        "    self.observation_space = list(itertools.product(range(self.N), range(self.M)))\n",
        "    # action space\n",
        "    self.action_space = [(0,1), (0,-1), (1,0), (-1,0)]\n",
        "\n",
        "    self.terminal_states = [(3,3)]\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = (0,0)\n",
        "    self.is_terminated = False\n",
        "    self.total_reward = 0\n",
        "\n",
        "  def get_transition_probaility(self, start_state, action, end_state):\n",
        "    if start_state in self.terminal_states:\n",
        "      return 0\n",
        "\n",
        "    expected_state = tuple(np.array(start_state) + np.array(action))\n",
        "    if expected_state == end_state:\n",
        "      return 1\n",
        "\n",
        "    if expected_state not in self.observation_space and start_state == end_state:\n",
        "      return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "  def get_reward(self, start_state, action, end_state):\n",
        "    if end_state in self.terminal_states:\n",
        "      return 10\n",
        "    else:\n",
        "      return -1\n",
        "\n",
        "  def step(self, action):\n",
        "    if self.state in self.terminal_states:\n",
        "      self.is_terminated = True\n",
        "      reward = np.nan\n",
        "      return self.state, reward, self.is_terminated\n",
        "\n",
        "    current_state = self.state\n",
        "    max_prob = 0\n",
        "    for possible_state in self.observation_space:\n",
        "      p = self.get_transition_probaility(current_state, action, possible_state)\n",
        "      if p > max_prob:\n",
        "        max_prob = p\n",
        "        next_state = possible_state\n",
        "\n",
        "    reward = self.get_reward(current_state, action, next_state)\n",
        "\n",
        "    self.state = next_state\n",
        "    self.total_reward += reward\n",
        "\n",
        "    return self.state, reward, self.is_terminated\n",
        "\n",
        "\n",
        "class RandomActionAgent:\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "\n",
        "  def policy(self):\n",
        "    action = random.choices(self.env.action_space)[0]\n",
        "    return\n",
        "\n",
        "  def train(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kk2wmPXtqp9M"
      },
      "outputs": [],
      "source": [
        "env = GridWorldEnv(5,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH0RL8n_tqdy",
        "outputId": "7d558e0c-ab35-468e-83f2-87b041487ef0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97T-E9kxrJ4",
        "outputId": "7914303d-33ab-49dc-c29d-152edef9e1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next state:  (1, 0)\n",
            "Reward:  -1\n"
          ]
        }
      ],
      "source": [
        "action = (1, 0)\n",
        "next_state, reward, is_terminated = env.step(action)\n",
        "\n",
        "print(\"Next state: \", next_state)\n",
        "print(\"Reward: \", reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryeIU82yHut"
      },
      "source": [
        "### Random walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomActionAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def policy(self):\n",
        "        action = random.choice(self.env.action_space)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfI0OeZ-4dkt",
        "outputId": "da027ccb-a0ed-4236-a475-c36ab980534e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-51"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.reset()\n",
        "agent = RandomActionAgent(env)\n",
        "\n",
        "while not env.is_terminated:\n",
        "  current_state = env.state\n",
        "  action =  agent.policy()\n",
        "  next_state, reward, is_terminated = env.step(action)\n",
        "  # print(current_state, action, reward, next_state)\n",
        "\n",
        "env.total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FBoOl5375EQw"
      },
      "outputs": [],
      "source": [
        "class DPAgent:\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.gamma = 1\n",
        "\n",
        "    self.v = dict(zip(self.env.observation_space, np.zeros(self.env.N*self.env.M)))\n",
        "    self.is_trained = False\n",
        "\n",
        "\n",
        "  def policy(self):\n",
        "    if not self.is_trained:\n",
        "      action = random.choices(self.env.action_space)[0]\n",
        "    else:\n",
        "      s = self.env.state\n",
        "      max = -np.inf\n",
        "      for a in self.env.action_space:\n",
        "        term = 0\n",
        "        for s_prime in self.env.observation_space:\n",
        "          term+= self.env.get_transition_probaility(s, a, s_prime) * (self.env.get_reward(s, a, s_prime) + self.gamma * self.v[s_prime])\n",
        "        if term > max:\n",
        "          max = term\n",
        "          action = a\n",
        "\n",
        "    return action\n",
        "\n",
        "  def train(self, iter_limit = 1000):\n",
        "\n",
        "    print(\"performing training...\")\n",
        "\n",
        "    self.v = dict(zip(self.env.observation_space, np.zeros(self.env.N*self.env.M)))\n",
        "\n",
        "    iter = 0\n",
        "    while iter< iter_limit:\n",
        "      for s in self.env.observation_space:\n",
        "        max = -np.inf\n",
        "        for a in self.env.action_space:\n",
        "          term2 = 0\n",
        "          for s_prime in self.env.observation_space:\n",
        "            term2+= self.env.get_transition_probaility(s, a, s_prime) * (self.env.get_reward(s, a, s_prime) + self.gamma*self.v[s_prime])\n",
        "          if term2 > max:\n",
        "            max = term2\n",
        "        self.v[s] = max\n",
        "      iter+=1\n",
        "\n",
        "    self.is_trained = True\n",
        "\n",
        "    print(np.array(list(self.v.values())).reshape(self.env.N, self.env.M))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19XWoAYY7NM-",
        "outputId": "35cb850f-1a78-44dc-a705-b3c84c3a6678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "performing training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 5.  6.  7.  8.  7.]\n",
            " [ 6.  7.  8.  9.  8.]\n",
            " [ 7.  8.  9. 10.  9.]\n",
            " [ 8.  9. 10.  0. 10.]\n",
            " [ 7.  8.  9. 10.  9.]]\n"
          ]
        }
      ],
      "source": [
        "env = GridWorldEnv(5,5)\n",
        "dp_agent = DPAgent(env)\n",
        "\n",
        "dp_agent.train(iter_limit = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDRdUYivBLZu",
        "outputId": "efd89f85-ade6-4c04-ff27-a583dc62262f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "performing training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 5.  6.  7.  8.  7.]\n",
            " [ 6.  7.  8.  9.  8.]\n",
            " [ 7.  8.  9. 10.  9.]\n",
            " [ 8.  9. 10.  0. 10.]\n",
            " [ 7.  8.  9. 10.  9.]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = GridWorldEnv(5,5)\n",
        "dp_agent = DPAgent(env)\n",
        "\n",
        "dp_agent.train(iter_limit = 100)\n",
        "\n",
        "env.reset()\n",
        "\n",
        "while not env.is_terminated:\n",
        "  current_state = env.state\n",
        "  action =  dp_agent.policy()\n",
        "  next_state, reward, is_terminated = env.step(action)\n",
        "  # print(current_state, action, reward, next_state)\n",
        "\n",
        "env.total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTsuYtlKC3t5"
      },
      "source": [
        "Create a heatmap of optimal state values of 10 X 10 gridworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZN9aO7pDBFK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
